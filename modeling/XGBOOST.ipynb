{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19b25b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc00678",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fake_name = \"fake20230912.npy\"\n",
    "train_data_o = np.array(pd.read_excel(\"../data/original_cohort_data.xlsx\", sheet_name=1))\n",
    "test_data_o = np.array(pd.read_excel(\"../data/original_cohort_data.xlsx\", sheet_name=2))\n",
    "fake_data_o = np.array(pd.read_excel(\"../data/synthetic_augmented_data.xlsx\", sheet_name=0))\n",
    "\n",
    "# 如果不用假数据 就把下面两行注释掉\n",
    "# fake_data_o = np.load(\"../data/%s\" % fake_name)\n",
    "train_data_o = np.concatenate((train_data_o,fake_data_o))\n",
    "train_x_o = train_data_o[:, 0: -1]\n",
    "train_y_o = train_data_o[:, -1]\n",
    "test_x_o = test_data_o[:, 0: -1]\n",
    "test_y_o = test_data_o[:, -1]\n",
    "\n",
    "train_x_o.shape, train_y_o.shape, test_x_o.shape, test_y_o.shape\n",
    "mms_x = MinMaxScaler()\n",
    "train_x = mms_x.fit_transform(train_x_o)\n",
    "test_x = mms_x.transform(test_x_o)\n",
    "\n",
    "mms_y = MinMaxScaler()\n",
    "train_y = mms_y.fit_transform(train_y_o[:,np.newaxis])\n",
    "test_y = mms_y.fit_transform(test_y_o[:,np.newaxis])\n",
    "print(train_data_o.shape,train_x_o.shape,  train_y_o.shape, test_x_o.shape,test_y_o.shape )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26edbf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_error_statistics(y_true, y_pred, label, scaler):\n",
    "    # Normalized error statistics\n",
    "    mean_error_norm = np.mean(y_true - y_pred)\n",
    "    std_error_norm = np.std(y_true - y_pred)\n",
    "    mae_norm = mean_absolute_error(y_true, y_pred)\n",
    "    mse_norm = np.mean((y_true - y_pred)**2)\n",
    "    rmse_norm = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    r2_norm = r2_score(y_true, y_pred)\n",
    "    \n",
    "    print(f\"{label}\\n归一化：        mean error:{mean_error_norm:.2f}  std error:{std_error_norm:.2f}  mae:{mae_norm:.2f}  mse:{mse_norm:.2f}  rmse:{rmse_norm:.2f}  r2:{r2_norm:.2f}\")\n",
    "\n",
    "    # Denormalized error statistics\n",
    "    y_true_denorm = scaler.inverse_transform(y_true)\n",
    "    y_pred_denorm = scaler.inverse_transform(y_pred)\n",
    "    mean_error_denorm = np.mean(y_true_denorm - y_pred_denorm)\n",
    "    std_error_denorm = np.std(y_true_denorm - y_pred_denorm)\n",
    "    mae_denorm = mean_absolute_error(y_true_denorm, y_pred_denorm)\n",
    "    mse_denorm = np.mean((y_true_denorm - y_pred_denorm)**2)\n",
    "    rmse_denorm = np.sqrt(mean_squared_error(y_true_denorm, y_pred_denorm))\n",
    "    r2_denorm = r2_score(y_true_denorm, y_pred_denorm)\n",
    "    print(f\"反归一化：      mean error:{mean_error_denorm:.2f}  std error:{std_error_denorm:.2f}  mae:{mae_denorm:.2f}  mse:{mse_denorm:.2f}  rmse:{rmse_denorm:.2f}  r2:{r2_denorm:.2f}\")\n",
    "\n",
    "# Placeholder for the best model\n",
    "\n",
    "# Placeholder for the best learning rate\n",
    "    # Placeholder for the best learning rate and best number of estimators\n",
    "# Placeholder for the best learning rate and best number of estimators\n",
    "best_learning_rate = None\n",
    "best_n_estimators = None  # Initialize here\n",
    "# Initialize best_val_loss with a large number\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "\n",
    "# 设置筛选迭代次数与学习率\n",
    "learning_rates = [0.01, 0.01, 0.001, 0.0001]\n",
    "n_estimators_list = [100, 200, 300, 400, 500]  # Add more values as needed\n",
    "# Initialize best_val_loss with a large number\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c245b8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for lr in learning_rates:\n",
    "    for n_estimators in n_estimators_list:\n",
    "        print(f\"Evaluating model with learning rate: {lr}, number of estimators: {n_estimators}\")\n",
    "\n",
    "    # 10-fold cross-validation\n",
    "    kf = KFold(n_splits=10)\n",
    "    fold_num = 1\n",
    "    val_losses = []\n",
    "\n",
    "    for train_index, val_index in kf.split(train_x):\n",
    "        # XGBoost model with specified learning rate, create a new instance for each fold\n",
    "        xgb_model = xgb.XGBRegressor(learning_rate=lr, n_estimators=n_estimators, tree_method='gpu_hist')\n",
    "\n",
    "        X_train, X_val = train_x[train_index], train_x[val_index]\n",
    "        y_train, y_val = train_y[train_index], train_y[val_index]\n",
    "\n",
    "        # Training the model\n",
    "        xgb_model.fit(X_train, y_train.ravel())\n",
    "\n",
    "        # Predictions\n",
    "        val_pred = xgb_model.predict(X_val)\n",
    "\n",
    "        # Mean Absolute Error for validation set\n",
    "        val_loss = mean_absolute_error(y_val, val_pred)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        print(f\"FOLD {fold_num}\")\n",
    "        print(f\"Validation loss: {val_loss}\")\n",
    "        fold_num += 1\n",
    "\n",
    "    # Average validation loss for this learning rate\n",
    "    avg_val_loss = np.mean(val_losses)\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        best_learning_rate = lr\n",
    "        best_n_estimators = n_estimators  # Update here\n",
    "print(f\"Best learning rate: {best_learning_rate}, Best number of estimators: {best_n_estimators}, validation loss: {best_val_loss}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb90899",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import xgboost as xgb\n",
    "import pydot\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# --- 路径设置 (相对路径) ---\n",
    "# 假设当前脚本在 modeling 文件夹下，回退一级找到 output\n",
    "output_dir = '../output'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# Split the original training data\n",
    "train_x, valid_x, train_y, valid_y = train_test_split(train_x, train_y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create DMatrix objects\n",
    "dtrain = xgb.DMatrix(train_x, label=train_y)\n",
    "dvalid = xgb.DMatrix(valid_x, label=valid_y)\n",
    "dtest = xgb.DMatrix(test_x, label=test_y)\n",
    "\n",
    "# Training parameters\n",
    "test_losses = []\n",
    "evals_result = {}\n",
    "params = {\n",
    "    'booster': 'gbtree',\n",
    "    'objective': 'reg:squarederror',\n",
    "    'eta': best_learning_rate,\n",
    "    # add other parameters here as needed\n",
    "}\n",
    "\n",
    "# Training\n",
    "final_xgb_model = xgb.train(\n",
    "    params,\n",
    "    dtrain,\n",
    "    num_boost_round=best_n_estimators,\n",
    "    evals=[(dtrain, 'train'), (dvalid, 'valid'), (dtest, 'test')],\n",
    "    evals_result=evals_result,\n",
    "    verbose_eval=False\n",
    ")\n",
    "\n",
    "test_losses = evals_result['test']['rmse']\n",
    "\n",
    "# Prepare Loss DataFrame\n",
    "loss_df = pd.DataFrame({\n",
    "    'Iteration': list(range(1, len(test_losses) + 1)),\n",
    "    'Loss': test_losses,\n",
    "    'Type': ['Test Loss'] * len(test_losses)\n",
    "})\n",
    "\n",
    "# --- 绘图 1: Test Loss (保存并显示) ---\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.lineplot(data=loss_df, x='Iteration', y='Loss', hue='Type')\n",
    "plt.title('Test Loss vs Iteration')\n",
    "\n",
    "# 保存图片到 output\n",
    "loss_plot_path = os.path.join(output_dir, 'test_loss_curve.png')\n",
    "plt.savefig(loss_plot_path, dpi=300, bbox_inches='tight')\n",
    "print(f\"Loss curve saved to: {loss_plot_path}\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Predictions\n",
    "train_pred_final = final_xgb_model.predict(dtrain)\n",
    "test_pred_final = final_xgb_model.predict(xgb.DMatrix(test_x))\n",
    "\n",
    "# --- 绘图 2: 决策树 (保存到 output) ---\n",
    "dot_data = xgb.to_graphviz(final_xgb_model, num_trees=0)\n",
    "\n",
    "# 定义保存路径 (使用相对路径)\n",
    "dot_file_path = os.path.join(output_dir, 'tree.dot')\n",
    "resized_dot_file_path = os.path.join(output_dir, 'tree_resized.dot')\n",
    "resized_png_file_path = os.path.join(output_dir, 'tree_visualization.png')\n",
    "\n",
    "# 写入 dot 文件\n",
    "with open(dot_file_path, 'w') as f:\n",
    "    f.write(dot_data.source)\n",
    "\n",
    "# Pydot 处理\n",
    "(graph,) = pydot.graph_from_dot_file(dot_file_path)\n",
    "graph.set('dpi', '300')\n",
    "graph.set_rankdir('LR')    # 保持从左到右\n",
    "graph.set_size('\"20,10!\"') # 保持之前的尺寸设置\n",
    "\n",
    "# 保存调整后的文件和图片\n",
    "graph.write_dot(resized_dot_file_path)\n",
    "graph.write_png(resized_png_file_path)\n",
    "\n",
    "print(f\"Decision tree saved to: {resized_png_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1c5a16",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import contextlib\n",
    "\n",
    "# --- 路径设置 ---\n",
    "output_dir = '../output'\n",
    "model_save_path = os.path.join(output_dir, 'final_xgb_model.json')\n",
    "performance_txt_path = os.path.join(output_dir, 'model_performance_metrics.txt')\n",
    "importance_plot_path = os.path.join(output_dir, 'feature_importance_plot.png')\n",
    "\n",
    "# Predictions\n",
    "train_pred_final = final_xgb_model.predict(dtrain)\n",
    "valid_pred_final = final_xgb_model.predict(dvalid)\n",
    "test_pred_final = final_xgb_model.predict(dtest)\n",
    "\n",
    "# 1. 保存模型到 output\n",
    "final_xgb_model.save_model(model_save_path)\n",
    "print(f\"Model saved to: {model_save_path}\")\n",
    "\n",
    "# 2. 将误差统计输出到 txt 文件 (同时在屏幕打印可选)\n",
    "# 打开文件准备写入\n",
    "with open(performance_txt_path, 'w') as f:\n",
    "    # 使用 redirect_stdout 将 print 的内容捕获到文件 f 中\n",
    "    with contextlib.redirect_stdout(f):\n",
    "        print(\"=== Model Performance Metrics ===\\n\")\n",
    "        print_error_statistics(train_y, train_pred_final[:, np.newaxis], 'train', mms_y)\n",
    "        print(\"\\n\" + \"-\"*30 + \"\\n\")\n",
    "        print_error_statistics(valid_y, valid_pred_final[:, np.newaxis], 'valid', mms_y)\n",
    "        print(\"\\n\" + \"-\"*30 + \"\\n\")\n",
    "        print_error_statistics(test_y, test_pred_final[:, np.newaxis], 'test', mms_y)\n",
    "\n",
    "print(f\"Performance metrics saved to: {performance_txt_path}\")\n",
    "\n",
    "# 3. 绘制并保存特征重要性图\n",
    "plt.figure(figsize=(15, 10))\n",
    "xgb.plot_importance(final_xgb_model, title='Feature importance')\n",
    "plt.title('Feature importance', fontsize=16)\n",
    "plt.xlabel('F score', fontsize=14)\n",
    "\n",
    "# 保存图片\n",
    "plt.savefig(importance_plot_path, dpi=300, bbox_inches='tight')\n",
    "print(f\"Feature importance plot saved to: {importance_plot_path}\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4453fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 路径设置 ---\n",
    "output_dir = '../output'\n",
    "importance_data_path = os.path.join(output_dir, 'feature_importance_data.txt')\n",
    "\n",
    "# 获取数据\n",
    "importances = final_xgb_model.get_score(importance_type='gain')\n",
    "importance_df = pd.DataFrame(list(importances.items()), columns=['feature', 'importance'])\n",
    "importance_df = importance_df.sort_values(by='importance', ascending=False)\n",
    "\n",
    "# 打印到控制台\n",
    "print(importance_df)\n",
    "\n",
    "# 保存到 output 文件夹下的 txt\n",
    "# 使用 to_string() 可以保持表格对齐的格式\n",
    "with open(importance_data_path, 'w') as f:\n",
    "    f.write(\"=== Feature Importance (Gain) ===\\n\\n\")\n",
    "    f.write(importance_df.to_string(index=False))\n",
    "\n",
    "print(f\"Feature importance data saved to: {importance_data_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db8770b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "\n",
    "# --- 1. 路径设置 ---\n",
    "input_file_path = \"../input/new_subject_input_template.xlsx\"\n",
    "model_load_path = \"../output/final_xgb_model.json\"\n",
    "result_output_path = \"../output/optimal_recommendation_result.txt\"\n",
    "\n",
    "# --- 2. 加载数据 ---\n",
    "if not os.path.exists(input_file_path):\n",
    "    print(f\"Error: Input file not found at {input_file_path}\")\n",
    "else:\n",
    "    # 读取数据\n",
    "    # 这一步非常重要：确保你的Excel里的列顺序，和训练时完全一致！\n",
    "    try:\n",
    "        # 尝试删除无关列 (序号 和 结果列)\n",
    "        data_predicting = pd.read_excel(input_file_path).drop([\"No.\", \"Rowing distance (m)\"], axis=1)\n",
    "    except KeyError:\n",
    "        # 容错：防止列名有细微差别\n",
    "        print(\"Warning: Standard columns not found, checking raw columns...\")\n",
    "        data_predicting = pd.read_excel(input_file_path)\n",
    "        if \"序号\" in data_predicting.columns:\n",
    "            data_predicting = data_predicting.drop([\"序号\"], axis=1)\n",
    "        if \"y\" in data_predicting.columns:\n",
    "            data_predicting = data_predicting.drop([\"y\"], axis=1)\n",
    "\n",
    "    # --- 3. 加载模型 ---\n",
    "    # 使用原生 Booster 加载\n",
    "    loaded_xgb_model = xgb.Booster() \n",
    "    try:\n",
    "        loaded_xgb_model.load_model(model_load_path)\n",
    "        print(f\"Successfully loaded model from: {model_load_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model: {e}\")\n",
    "\n",
    "    # --- 4. 构造枚举法预测 ---\n",
    "    data_list = []\n",
    "    \n",
    "    # 提取体重数据，用于计算摄入量的绝对值\n",
    "    # 这里我们使用英文列名来定位数据，进行计算\n",
    "    weight_col = data_predicting[\"Weight (kg)\"] \n",
    "\n",
    "    # 循环枚举 CHO 摄入率 (0.5 - 1.2)\n",
    "    for i in np.arange(0.5, 1.21, 0.01):\n",
    "        \n",
    "        # 1. 更新 CHO 摄入速率\n",
    "        data_predicting[\"CHO intake rate (g/kg/h)\"] = i\n",
    "        \n",
    "        # 2. 联动更新其他营养素 (基于公式)\n",
    "        # 这里的英文列名必须和你Excel里的一致\n",
    "        data_predicting[\"Fat intake (g)\"] = 0.024 * i * weight_col\n",
    "        data_predicting[\"Sodium intake (mg)\"] = 0.0026643 * i * weight_col\n",
    "        data_predicting[\"Magnesium intake (mg)\"] = 0.00262 * i * weight_col\n",
    "        data_predicting[\"Calcium intake (mg)\"] = 0.0038383 * i * weight_col\n",
    "\n",
    "        # 3. 转换为 Numpy 数组\n",
    "        # 关键修改：直接取 values，不查 feature_names\n",
    "        # 只要 input excel 的列顺序是对的，这就没问题\n",
    "        tmp = data_predicting.values \n",
    "        data_list.append(tmp)\n",
    "\n",
    "    data_list = np.concatenate(data_list)\n",
    "    \n",
    "    # --- 进行归一化 ---\n",
    "    # 假设 mms_x 和 mms_y 还在内存中 (如果你重启了内核，需要重新定义它们)\n",
    "    x = mms_x.transform(data_list)\n",
    "\n",
    "    # --- 利用模型预测 ---\n",
    "    # 关键修改：因为模型里没有存名字，这里只给数据 (DMatrix(x))\n",
    "    dtest_input = xgb.DMatrix(x)\n",
    "    prediction = loaded_xgb_model.predict(dtest_input)\n",
    "\n",
    "    # --- 5. 找到最优解 ---\n",
    "    max_indice = np.argmax(prediction)\n",
    "    max_y_raw = prediction[max_indice]\n",
    "    max_x_raw = x[max_indice]\n",
    "    \n",
    "    # 反归一化获取真实值\n",
    "    max_x_original_scale = mms_x.inverse_transform(max_x_raw.reshape(1, -1))[0]\n",
    "    \n",
    "    # 获取最佳 CHO 摄入速率\n",
    "    # 假设 CHO 在你的 45 个指标中排在第 2 个 (索引为 1)\n",
    "    # 因为我们现在没法用名字查，只能按位置查。\n",
    "    # 根据你提供的列表：Previous meal (0), CHO intake rate (1)... \n",
    "    max_x2 = max_x_original_scale[1] \n",
    "    \n",
    "    # 获取最佳预测成绩 (y)\n",
    "    max_y = mms_y.inverse_transform(np.array([max_y_raw]).reshape(1, -1))[0][0]\n",
    "\n",
    "    # --- 6. 计算 PRO 摄入速率 ---\n",
    "    max_pro = max_x2 / 4.0\n",
    "\n",
    "    # --- 7. 格式化输出与保存 ---\n",
    "    output_content = (\n",
    "        \"=== Personalized Recommendation Result ===\\n\"\n",
    "        f\"Predicted Max Performance (Rowing Distance): {max_y:.2f} m\\n\"\n",
    "        f\"Optimal CHO Intake Rate: {max_x2:.2f} g/kg/h\\n\"\n",
    "        f\"Optimal PRO Intake Rate: {max_pro:.2f} g/kg/h\\n\"\n",
    "    )\n",
    "\n",
    "    print(output_content)\n",
    "\n",
    "    with open(result_output_path, \"w\") as f:\n",
    "        f.write(output_content)\n",
    "    \n",
    "    print(f\"Result saved to: {result_output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e923c51e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (xgb_gpu_env)",
   "language": "python",
   "name": "xgb_gpu_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
